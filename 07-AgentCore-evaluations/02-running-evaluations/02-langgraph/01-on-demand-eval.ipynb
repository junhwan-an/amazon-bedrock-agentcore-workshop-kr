{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df90111-38c8-4032-889a-0ee5a10f6132",
   "metadata": {},
   "source": [
    "## AgentCore Evaluations - LangGraph를 위한 온디맨드 평가\n",
    "\n",
    "이 튜토리얼에서는 LangGraph agent에 적용되는 AgentCore Evaluations의 온디맨드 평가를 사용하는 방법을 배웁니다.\n",
    "\n",
    "이 실습을 실행하려면 먼저 [00-prereqs](../../00-prereqs) 폴더의 코드를 사용하여 LangGraph agent를 생성하고 [01-creating-custom-evaluators](../../01-creating-custom-evaluators)의 코드를 사용하여 커스텀 evaluator를 생성해야 합니다.\n",
    "\n",
    "### 학습 내용\n",
    "- AgentCore Starter toolkit을 사용하여 trace에 대한 온디맨드 평가를 실행하는 방법\n",
    "\n",
    "### 튜토리얼 세부 정보\n",
    "\n",
    "| Information         | Details                                                                       |\n",
    "|:--------------------|:------------------------------------------------------------------------------|\n",
    "| Tutorial type       | 온디맨드 evaluator(내장 및 커스텀)를 사용한 LangGraph agent 평가      |\n",
    "| Tutorial components | 내장 및 커스텀 evaluator를 사용한 평가 실행                        |\n",
    "| Tutorial vertical   | Cross-vertical                                                                |\n",
    "| Example complexity  | Easy                                                                          |\n",
    "| SDK used            | Amazon Bedrock AgentCore Starter toolkit                                      |\n",
    "\n",
    "### 온디맨드 평가\n",
    "\n",
    "온디맨드 평가는 선택한 span 세트를 직접 분석하여 특정 agent 상호작용을 평가하는 유연한 방법을 제공합니다. 프로덕션 트래픽을 지속적으로 모니터링하는 온라인 평가와 달리, 온디맨드 평가를 사용하면 언제든지 선택한 상호작용에 대한 타겟 평가를 수행할 수 있습니다.\n",
    "\n",
    "온디맨드 평가를 사용하면 span, trace 또는 session ID를 제공하여 평가하려는 정확한 span, trace 또는 session을 지정합니다. AgentCore Starter toolkit을 사용하면 session의 모든 trace를 자동으로 평가할 수도 있습니다.\n",
    "\n",
    "그런 다음 커스텀 evaluator 또는 내장 evaluator를 agent의 상호작용에 적용할 수 있습니다. 이 평가 유형은 특정 고객 상호작용을 조사하거나, 보고된 문제에 대한 수정 사항을 검증하거나, 품질 개선을 위해 과거 데이터를 분석해야 할 때 특히 유용합니다. 평가 요청을 제출하면 서비스는 지정된 span만 처리하고 분석을 위한 상세한 결과를 제공합니다.\n",
    "\n",
    "### Agent에서 AgentCore Observability로 trace 생성\n",
    "\n",
    "AgentCore Observability는 상세한 실행 데이터를 캡처하고 구조화하기 위한 기반으로 [OpenTelemetry (OTEL)](https://opentelemetry.io/) trace를 활용하여 호출 중 agent 동작에 대한 포괄적인 가시성을 제공합니다. AgentCore는 [AWS Distro for OpenTelemetry (ADOT)](https://aws-otel.github.io/)를 사용하여 다양한 agent 프레임워크에서 여러 유형의 OTEL trace를 계측합니다.\n",
    "\n",
    "Agent가 AgentCore Runtime에서 호스팅되는 경우(이 튜토리얼의 agent처럼), AgentCore Observability 계측은 최소한의 구성으로 자동으로 수행됩니다. `requirements.txt`에 `aws-opentelemetry-distro`를 포함하기만 하면 AgentCore Runtime이 OTEL 구성을 자동으로 처리합니다. Agent가 AgentCore Runtime에서 실행되지 않는 경우, AgentCore Observability에서 사용할 수 있도록 ADOT로 계측해야 합니다. 원격 측정 데이터를 CloudWatch로 전달하도록 환경 변수를 구성하고 OpenTelemetry 계측으로 agent를 실행해야 합니다.\n",
    "\n",
    "프로세스는 다음과 같습니다:\n",
    "\n",
    "![session_traces](../../images/observability_traces.png)\n",
    "\n",
    "Session trace가 AgentCore Observability에서 사용 가능해지면 AgentCore Evaluations를 사용하여 agent의 동작을 평가할 수 있습니다.\n",
    "\n",
    "### 온디맨드 평가가 trace와 작동하는 방식\n",
    "\n",
    "온디맨드 평가에서 agent가 호출되고 AgentCore Observability에서 trace를 생성합니다. 이러한 trace는 session에 매핑되고 로그는 Amazon CloudWatch Log 그룹에서 사용할 수 있게 됩니다. 온디맨드 평가를 사용하면 개발자가 사용할 session 또는 trace를 결정하고 trace 콘텐츠를 평가할 메트릭과 함께 이를 AgentCore Evaluations에 입력으로 보냅니다. 프로세스는 다음과 같습니다:\n",
    "\n",
    "\n",
    "![session_traces](../../images/on_demand_evaluations.png)\n",
    "\n",
    "### 이전 튜토리얼에서 정보 검색\n",
    "\n",
    "이 튜토리얼에서는 사전 요구 사항 튜토리얼에서 AgentCore Runtime에 배포된 LangGraph agent를 사용합니다. 사전 구축된 메트릭과 `01-creating-custom-metrics` 튜토리얼에서 생성한 `response_quality` 메트릭으로 평가합니다. agent 및 evaluator 정보를 검색해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6877d3a-d584-4805-bbec-a8629ac288fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 노트북에서 저장된 변수들을 불러옴\n",
    "%store -r launch_result_langgraph\n",
    "%store -r session_id_langgraph\n",
    "%store -r evaluator_id\n",
    "try:\n",
    "    print(\"Agent Id:\", launch_result_langgraph.agent_id)\n",
    "    print(\"Agent ARN:\", launch_result_langgraph.agent_arn)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing launch results from your LangGraph agent. Please run 00-prereqs before executing this lab\"\"\")\n",
    "\n",
    "try:\n",
    "    print(\"Session id:\", session_id_langgraph)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing session id from your LangGraph agent. Please run 00-prereqs before executing this lab\"\"\")\n",
    "\n",
    "try:\n",
    "    print(\"Evaluator id:\", evaluator_id)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing custom evaluator id. Please run 01-creating-custom-evaluators before executing this lab\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c895803-39e1-443c-95b1-df9d0bf6d801",
   "metadata": {},
   "source": [
    "### AgentCore Evaluations 클라이언트 초기화\n",
    "\n",
    "이제 AgentCore Starter toolkit에서 AgentCore Evaluations 클라이언트를 초기화해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation, Observability\n",
    "import os\n",
    "import json\n",
    "from boto3.session import Session\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7afa7-b1ca-43bf-a28b-6c1fcc8cdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS 세션 생성 및 리전 정보 가져오기\n",
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312c878-11ba-4cda-9afb-b5cfcabc4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgentCore Evaluation 클라이언트 초기화\n",
    "eval_client = Evaluation(region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963ec2a-f2ad-45d4-9830-685dfbee024b",
   "metadata": {},
   "source": [
    "### 평가 실행\n",
    "\n",
    "AgentCore Evaluations를 실행하려면 session, trace 또는 span 정보를 제공해야 합니다. 이전 튜토리얼에서 본 것처럼 다양한 메트릭은 agent trace에서 다양한 수준의 정보를 요구합니다.\n",
    "\n",
    "![metrics level](../../images/metrics_per_level.png)\n",
    "\n",
    "AWS의 SDK 중 하나를 사용하는 경우 trace를 직접 처리해야 합니다. AgentCore Starter toolkit은 이 프로세스를 단순화하고 session id 또는 trace id를 기반으로 trace를 처리합니다. 이렇게 하면 `run` 상호작용 중에 AgentCore starter toolkit evaluator 클라이언트에 session id를 제공할 수 있으며 SDK는 trace 및 span 수준 메트릭에 대해 해당 session의 모든 trace를 추출하고 평가합니다. 선택적으로 평가 결과를 저장할 출력 파일의 이름을 제공할 수도 있습니다.\n",
    "\n",
    "### Goal Success Rate\n",
    "\n",
    "이제 agent의 Goal Success Rate를 평가해 보겠습니다. agent에게 다음 질문을 했던 것을 기억하세요:\n",
    "\n",
    "* What is the weather now?\n",
    "* How much is 2+2?\n",
    "* Can you tell me the capital of the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9a1be-5e03-4fa3-8bf4-6a042b4ef23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal Success Rate 평가 실행 (session 레벨 메트릭)\n",
    "goal_sucess_results = eval_client.run(\n",
    "    agent_id=launch_result_langgraph.agent_id,\n",
    "    session_id=session_id_langgraph, \n",
    "    evaluators=[\"Builtin.GoalSuccessRate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6a01a-818f-45fb-9f23-456887685d12",
   "metadata": {},
   "source": [
    "이제 evaluator의 결과를 이해해 보겠습니다. 결과 객체에는 수행된 평가에 대한 정보(session_id, trace_id, input_data)와 평가 결과가 포함됩니다.\n",
    "\n",
    "평가 결과에는 evaluator 정보(id, name, ARN), 평가 값, 평가 레이블, 평가 설명 및 평가 작업에 대한 일부 추가 컨텍스트(spanContext, token_usage, ...)가 포함됩니다.\n",
    "\n",
    "평가 응답을 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9666eb-9c56-4bc3-acbb-d3c70ffee597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 결과 출력 (값, 설명, 토큰 사용량, 컨텍스트)\n",
    "for result in goal_sucess_results.results:\n",
    "    information = f\"\"\"\n",
    "    Goal Success: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9cc0e-66c5-42fa-86e7-c568618aa7bd",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "이제 trace 수준 메트릭인 Correctness에 대해 동일한 session을 분석해 보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000cd07-885d-4bc6-8d4f-aa9de3096d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctness 평가 실행 (trace 레벨 메트릭)\n",
    "correctness_results = eval_client.run(\n",
    "    agent_id=launch_result_langgraph.agent_id,\n",
    "    session_id=session_id_langgraph, \n",
    "    evaluators=[\"Builtin.Correctness\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb2ef19-74e3-4d2a-8a96-5a08c934c153",
   "metadata": {},
   "source": [
    "이제 evaluator의 결과를 이해해 보겠습니다. 이 경우 correctness는 trace 수준에서 평가되므로 각 trace는 자체 평가 결과를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752db8e-9089-4f4a-ad86-408758740ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 trace별 correctness 결과 출력\n",
    "for result in correctness_results.results:\n",
    "    information = f\"\"\"\n",
    "    Correctness: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ba398-3c2a-44c2-9288-65acd3a67dd3",
   "metadata": {},
   "source": [
    "### Tool selection accuracy 및 parameter selection accuracy\n",
    "\n",
    "이제 tool 및 parameter 선택에 대해 agent를 평가해 보겠습니다. 두 메트릭 모두 span 수준에서 평가됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e30a77-8309-4c62-87e4-deb96d1178a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 선택 및 Parameter 선택 정확도 평가 (span 레벨 메트릭)\n",
    "parameter_results = eval_client.run(\n",
    "    agent_id=launch_result_langgraph.agent_id,\n",
    "    session_id=session_id_langgraph, \n",
    "    evaluators=[\"Builtin.ToolParameterAccuracy\", \"Builtin.ToolSelectionAccuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b53ea-6974-4731-b73a-36242e72d720",
   "metadata": {},
   "source": [
    "이제 결과를 분석해 보겠습니다. 이 경우 동일한 실행에서 두 가지 다른 메트릭으로 session을 평가하고 있습니다. 즉, 이제 어떤 evaluator가 각 응답을 생성하는지 알아야 합니다. 결과의 `evaluator_name` 속성으로 이를 수행할 수 있습니다. agent가 tool을 얼마나 잘 사용했는지 살펴보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b645bc-4897-4200-a25b-871786c1fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 evaluator 결과 출력 (evaluator_name으로 구분)\n",
    "for result in parameter_results.results:\n",
    "    information = f\"\"\"\n",
    "    Metric: {result.evaluator_name}\n",
    "    Value: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650fac68-1adb-4ce6-ab07-770405ac5532",
   "metadata": {},
   "source": [
    "### 커스텀 evaluator 사용\n",
    "\n",
    "이제 session, trace 및 span 수준에서 evaluator를 사용했으므로 커스텀 메트릭을 사용하여 응답 품질을 평가해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282eb44-0611-4e9d-96ad-2b009f62e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 evaluator로 응답 품질 평가\n",
    "custom_results = eval_client.run(\n",
    "    agent_id=launch_result_langgraph.agent_id,\n",
    "    session_id=session_id_langgraph, \n",
    "    evaluators=[evaluator_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e2b08-8007-49e2-af75-3aeef58956c3",
   "metadata": {},
   "source": [
    "이제 평가 결과를 살펴보겠습니다. 이 경우 다음 지침이 있는 agent를 평가하고 있습니다:\n",
    "\n",
    "```\n",
    "You're a helpful assistant. You can do simple math calculation, and tell the weather.\n",
    "```\n",
    "\n",
    "평가 메트릭의 경우 평가 지침에 명시된 대로 범위를 벗어난 agent에 대해 `Very Poor` 품질로 페널티를 부과합니다:\n",
    "\n",
    "```\n",
    "...\n",
    "**IMPORTANT**: A response quality can only be high if the agent remains in its original scope. Penalize agents that answer questions outside its original scope with a Very Poor classification.\n",
    "...\n",
    "```\n",
    "\n",
    "다음 질문을 평가하고 있으므로:\n",
    "\n",
    "* What is the weather now?\n",
    "* How much is 2+2?\n",
    "* Can you tell me the capital of the US?\n",
    "\n",
    "마지막 질문에 대해 agent가 `Very Poor` 평가를 받을 것으로 예상합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42066be3-765e-4ec1-a728-e8da71197b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 evaluator 결과 출력\n",
    "for result in custom_results.results:\n",
    "    information = f\"\"\"\n",
    "    Metric: {result.evaluator_name}\n",
    "    Value: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c96505-abb6-4aa8-90cf-f047a5155435",
   "metadata": {},
   "source": [
    "### 평가 결과 저장\n",
    "\n",
    "AgentCore starter toolkit은 구조화된 출력 파일에 agent 평가 결과를 저장하는 데도 도움이 됩니다. 이를 위해 실행 중에 `ouput` 매개변수를 제공하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17db14a-9bde-4ad9-95aa-c0ecc2db0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 결과를 JSON 파일로 저장 (output 파라미터 사용)\n",
    "save_results = eval_client.run(\n",
    "    agent_id=launch_result_langgraph.agent_id,\n",
    "    session_id=session_id_langgraph, \n",
    "    evaluators=[\n",
    "        evaluator_id\n",
    "    ],\n",
    "    output=\"evals_results/output.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026bfcb6-0bb7-49e5-add2-e9ca3c677871",
   "metadata": {},
   "source": [
    "### 축하합니다!\n",
    "\n",
    "이제 온디맨드 기능으로 agent를 평가했습니다. 다음 튜토리얼에서는 온라인 evaluator를 설정하고 agent와 연결하여 프로덕션 환경에서 agent 평가를 자동화합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
