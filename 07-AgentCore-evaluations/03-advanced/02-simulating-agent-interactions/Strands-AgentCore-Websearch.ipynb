{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Simulator를 사용한 AgentCore 온라인 Evaluation\n",
    "\n",
    "**Pipeline:** AgentCore Online Eval Setup → DatasetGenerator → ActorSimulator → Agent 호출 → AgentCore가 CloudWatch를 통해 평가\n",
    "\n",
    "이 노트북:\n",
    "1. `eval_config.py`에서 설정을 로드합니다\n",
    "2. 내장 메트릭을 사용하여 AgentCore 온라인 evaluation을 설정합니다\n",
    "3. DatasetGenerator를 사용하여 테스트 케이스를 생성합니다\n",
    "4. actor simulator를 실행하여 멀티턴 대화로 agent를 호출합니다\n",
    "5. AgentCore가 CloudWatch를 통해 자동으로 추적 및 평가합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from strands_evals import ActorSimulator, Case  # Strands evaluation 라이브러리\n",
    "from strands_evals.generators import DatasetGenerator  # 테스트 케이스 자동 생성\n",
    "from strands_eval_config import *  # 외부 설정 파일에서 변수 import\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
    "print(\"Configuration loaded from eval_config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AgentCore 온라인 Evaluation 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgentCore evaluation controlplane client 생성\n",
    "evaluation_client = boto3.client(\n",
    "    'agentcore-evaluation-controlplane',\n",
    "    region_name=AWS_REGION,\n",
    ")\n",
    "\n",
    "# 온라인 evaluation 설정 생성\n",
    "create_config_response = evaluation_client.create_online_evaluation_config(\n",
    "    onlineEvaluationConfigName=EVAL_CONFIG_NAME+'_3',\n",
    "    description=EVAL_DESCRIPTION,\n",
    "    rule={\n",
    "        \"samplingConfig\": {\"samplingPercentage\": SAMPLING_PERCENTAGE},  # 샘플링 비율 설정\n",
    "        \"sessionConfig\": {\"sessionTimeoutMinutes\": SESSION_TIMEOUT_MINUTES}  # 세션 타임아웃\n",
    "    },\n",
    "    dataSourceConfig={\n",
    "        \"cloudWatchLogs\": {  # CloudWatch Logs에서 데이터 수집\n",
    "            \"logGroupNames\": [LOG_GROUP_NAME],\n",
    "            \"serviceNames\": [SERVICE_NAME]\n",
    "        }\n",
    "    },\n",
    "    evaluators=[{\"evaluatorId\": evaluator_id} for evaluator_id in EVALUATORS],  # 평가자 목록\n",
    "    evaluationExecutionRoleArn=EVALUATION_ROLE_ARN,\n",
    "    enableOnCreate=True  # 생성 즉시 활성화\n",
    ")\n",
    "\n",
    "config_id = create_config_response['onlineEvaluationConfigId']\n",
    "config_details = evaluation_client.get_online_evaluation_config(onlineEvaluationConfigId=config_id)\n",
    "\n",
    "print(f\"Created config: {config_id}\")\n",
    "print(f\"Status: {config_details['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AgentCore Runtime Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_client = boto3.client('bedrock-agentcore', region_name=AWS_REGION)\n",
    "\n",
    "def invoke_agentcore(user_message):\n",
    "    # AgentCore runtime 호출\n",
    "    boto3_response = agentcore_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=AGENT_ARN,\n",
    "        qualifier=QUALIFIER,\n",
    "        payload=json.dumps({\"prompt\": user_message})\n",
    "    )\n",
    "    \n",
    "    content = []\n",
    "    # 스트리밍 응답 처리\n",
    "    if \"text/event-stream\" in boto3_response.get(\"contentType\", \"\"):\n",
    "        for line in boto3_response[\"response\"].iter_lines(chunk_size=1):\n",
    "            if line:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if line.startswith(\"data: \"):\n",
    "                    line = line[6:]  # \"data: \" 접두사 제거\n",
    "                    content.append(line)\n",
    "    else:\n",
    "        # 비스트리밍 응답 처리\n",
    "        events = []\n",
    "        for event in boto3_response.get(\"response\", []):\n",
    "            events.append(event)\n",
    "        if events:\n",
    "            content.append(json.loads(events[0].decode(\"utf-8\")))\n",
    "    \n",
    "    return \"\\n\".join(str(c) for c in content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 테스트 케이스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetGenerator 초기화 (입력/출력 타입 모두 str)\n",
    "generator = DatasetGenerator[str, str](str, str)\n",
    "\n",
    "# Agent 특성을 설명하는 task description 생성\n",
    "task_description = f\"\"\"\n",
    "Task: {AGENT_CAPABILITIES}\n",
    "Limitations: {AGENT_LIMITATIONS}\n",
    "Available tools: {', '.join(AGENT_TOOLS)}\n",
    "Complexity: {AGENT_COMPLEXITY}\n",
    "\"\"\"\n",
    "\n",
    "# 테스트 케이스 자동 생성 (비동기)\n",
    "dataset = await generator.from_scratch_async(\n",
    "    topics=AGENT_TOPICS,\n",
    "    task_description=task_description,\n",
    "    num_cases=NUM_TEST_CASES\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(dataset.cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 테스트 케이스 미리보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, case in enumerate(dataset.cases, 1):\n",
    "    print(f\"\\nCase {i}: {case.input}\")\n",
    "    print(f\"Expected: {case.expected_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_function(case: Case) -> str:\n",
    "    # ActorSimulator 생성 (사용자 역할 시뮬레이션)\n",
    "    user_sim = ActorSimulator.from_case_for_user_simulator(case=case, max_turns=MAX_TURNS)\n",
    "    \n",
    "    user_message = case.input\n",
    "    final_response = \"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Case: {case.input}\")\n",
    "    print(f\"Expected: {case.expected_output}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    turn = 1\n",
    "    # 멀티턴 대화 시뮬레이션\n",
    "    while user_sim.has_next():\n",
    "        print(f\"\\nTurn {turn}: {user_message}\")\n",
    "        agent_response = invoke_agentcore(user_message)\n",
    "        final_response = agent_response\n",
    "        print(f\"Agent: {agent_response[:200]}...\")\n",
    "        \n",
    "        # 다음 사용자 메시지 생성\n",
    "        user_result = user_sim.act(agent_response)\n",
    "        user_message = str(user_result.structured_output.message)\n",
    "        turn += 1\n",
    "    \n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# 모든 테스트 케이스 실행\n",
    "for i, case in enumerate(dataset.cases, 1):\n",
    "    print(f\"\\n\\n{'#'*80}\")\n",
    "    print(f\"# Running Test Case {i}/{len(dataset.cases)}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    try:\n",
    "        response = task_function(case)\n",
    "        results.append({\n",
    "            \"case_number\": i,\n",
    "            \"input\": case.input,\n",
    "            \"expected\": case.expected_output,\n",
    "            \"actual\": response,\n",
    "            \"status\": \"success\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        results.append({\n",
    "            \"case_number\": i,\n",
    "            \"input\": case.input,\n",
    "            \"expected\": case.expected_output,\n",
    "            \"actual\": str(e),\n",
    "            \"status\": \"error\"\n",
    "        })\n",
    "\n",
    "# 결과 요약 출력\n",
    "print(f\"\\n\\nCompleted {len(results)} test cases\")\n",
    "print(f\"Successful: {sum(1 for r in results if r['status'] == 'success')}\")\n",
    "print(f\"Errors: {sum(1 for r in results if r['status'] == 'error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 결과 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 끝"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
