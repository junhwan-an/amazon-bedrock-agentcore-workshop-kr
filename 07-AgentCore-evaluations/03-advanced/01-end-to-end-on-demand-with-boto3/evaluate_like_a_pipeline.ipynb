{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgentCore 온라인 Evaluation\n",
    "\n",
    "프로그래밍 방식 테스트를 위한 Agent 호출 및 evaluation 워크플로우."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from IPython.display import Markdown, display\n",
    "# utils 모듈에서 evaluation 관련 헬퍼 함수들 import\n",
    "from utils import (\n",
    "    EvaluationClient,\n",
    "    generate_session_id,\n",
    "    invoke_and_evaluate,\n",
    ")\n",
    "\n",
    "# Set your AWS Credentials\n",
    "\n",
    "# os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
    "# os.environ['AWS_SESSION_TOKEN'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_ID = \"strands_claude_eval\"\n",
    "# ARN 형식: <YOUR_ACCOUNT_ID>를 실제 AWS 계정 ID로 교체 필요\n",
    "AGENT_ARN = f\"arn:aws:bedrock-agentcore:us-east-1:<YOUR_ACCOUNT_ID>:runtime/{AGENT_ID}\"\n",
    "REGION = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"my_experiment_v1\"\n",
    "\n",
    "# None으로 설정 시 13개 evaluator 모두 실행 (comprehensive mode)\n",
    "# 특정 리스트(예: FLEXIBLE_EVALUATORS)로 설정 시 해당 evaluator만 실행\n",
    "EXPERIMENT_EVALUATORS = None  # Runs all 13 evaluators\n",
    "\n",
    "EXPERIMENT_SCOPE = \"session\"  # EXPERIMENT_EVALUATORS가 None일 때는 무시됨\n",
    "EXPERIMENT_DELAY = 120 # trace가 AgentCore observability에 반영되기까지 최소 120초 필요\n",
    "\n",
    "planned_session = generate_session_id() \n",
    "\n",
    "# metadata는 선택사항이지만 추적을 위해 권장\n",
    "EXPERIMENT_PROMPTS = [\n",
    "    {\"prompt\": \"What is 2 + 2?\", \"session_id\": \"\", \"metadata\": {\"category\": \"math\"}},\n",
    "    {\"prompt\": \"What is the capital of France?\", \"session_id\": \"\", \"metadata\": {\"category\": \"geography\"}},\n",
    "    {\"prompt\": \"Tell me about quantum physics\", \"session_id\": \"\", \"metadata\": {\"category\": \"science\"}},\n",
    "    {\"prompt\": \"Hello, can you help me with math?\", \"session_id\": planned_session, \"metadata\": {\"turn\": 1}},\n",
    "    {\"prompt\": \"What is 15 * 23?\", \"session_id\": planned_session, \"metadata\": {\"turn\": 2}},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AgentCore client\n",
    "agentcore_client = boto3.client('bedrock-agentcore', region_name=REGION)\n",
    "\n",
    "# Initialize Evaluation client (utils 모듈의 커스텀 클래스)\n",
    "eval_client = EvaluationClient(\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session과 span 모두에서 사용 가능한 evaluator 목록\n",
    "FLEXIBLE_EVALUATORS = [\n",
    "    \"Builtin.Correctness\",\n",
    "    \"Builtin.Faithfulness\",\n",
    "    \"Builtin.Helpfulness\",\n",
    "    \"Builtin.ResponseRelevance\",\n",
    "    \"Builtin.Conciseness\",\n",
    "    \"Builtin.Coherence\",\n",
    "    \"Builtin.InstructionFollowing\",\n",
    "    \"Builtin.Refusal\",\n",
    "    \"Builtin.Harmfulness\",\n",
    "    \"Builtin.Stereotyping\"\n",
    "]\n",
    "\n",
    "# session scope에서만 사용 가능한 evaluator\n",
    "SESSION_ONLY_EVALUATORS = [\"Builtin.GoalSuccessRate\"]\n",
    "\n",
    "# span scope에서만 사용 가능한 evaluator (tool 관련 평가)\n",
    "SPAN_ONLY_EVALUATORS = [\n",
    "    \"Builtin.ToolSelectionAccuracy\",\n",
    "    \"Builtin.ToolParameterAccuracy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results = []\n",
    "\n",
    "# 실행할 evaluator 개수 계산 (None이면 전체 13개)\n",
    "eval_count = len(EXPERIMENT_EVALUATORS) if EXPERIMENT_EVALUATORS else 13\n",
    "print(f\"Experiment: {EXPERIMENT_NAME} | Prompts: {len(EXPERIMENT_PROMPTS)} | Evaluators: {eval_count}\\n\")\n",
    "\n",
    "for i, config in enumerate(EXPERIMENT_PROMPTS, 1):\n",
    "    prompt_text = config[\"prompt\"]\n",
    "    session_id = config.get(\"session_id\", \"\")\n",
    "    metadata = config.get(\"metadata\", {})\n",
    "        \n",
    "    try:\n",
    "        # agent 호출 및 evaluation 실행 (utils 모듈의 헬퍼 함수)\n",
    "        returned_session_id, content, results = invoke_and_evaluate(\n",
    "            agentcore_client=agentcore_client,\n",
    "            eval_client=eval_client,\n",
    "            agent_arn=AGENT_ARN,\n",
    "            agent_id=AGENT_ID,\n",
    "            region=REGION,\n",
    "            prompt=prompt_text,\n",
    "            experiment_name=EXPERIMENT_NAME,\n",
    "            session_id=session_id,\n",
    "            metadata=metadata,\n",
    "            evaluators=EXPERIMENT_EVALUATORS,\n",
    "            scope=EXPERIMENT_SCOPE,\n",
    "            delay=EXPERIMENT_DELAY,\n",
    "            flexible_evaluators=FLEXIBLE_EVALUATORS,\n",
    "            session_only_evaluators=SESSION_ONLY_EVALUATORS,\n",
    "            span_only_evaluators=SPAN_ONLY_EVALUATORS\n",
    "        )\n",
    "        \n",
    "        # agent 응답을 Markdown 형식으로 출력\n",
    "        if content:\n",
    "            display(Markdown(str(content[0])))\n",
    "        \n",
    "        batch_results.append({\n",
    "            \"session_id\": returned_session_id,\n",
    "            \"prompt\": prompt_text,\n",
    "            \"results\": results\n",
    "        })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")\n",
    "        batch_results.append({\"prompt\": prompt_text, \"error\": str(e)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
