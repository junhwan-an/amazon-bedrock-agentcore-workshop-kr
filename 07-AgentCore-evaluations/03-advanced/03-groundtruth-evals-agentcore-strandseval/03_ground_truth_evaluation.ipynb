{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Ground Truth Evaluation\n\n이 노트북은 Strands Evals(확장 가능한 LLM 기반 평가 프레임워크)를 사용하여 agent 응답을 ground truth(예상 출력)와 비교 평가합니다. rubric 전용 평가와 달리, ground truth 평가는 실제 출력을 미리 정의된 정답과 비교합니다.\n\n**사용 사례:**\n- 회귀 테스트: agent 변경이 알려진 정상 응답을 손상시키지 않는지 확인\n- 품질 벤치마킹: agent가 예상 동작과 얼마나 일치하는지 측정\n- 학습 데이터 검증: agent 출력을 큐레이션된 예제와 비교 검증\n\n**두 개의 데이터 소스 (별도 파일):**\n1. **Traces 파일** (`demo_traces.json`): AgentCore Observability의 실제 agent 응답 포함\n2. **Ground Truth 파일** (`demo_ground_truth.json`): 각 trace에 대한 예상 출력 포함\n\n노트북은 이 파일들을 `trace_id`로 병합하여 실제와 예상을 비교합니다.\n\n**두 가지 모드:**\n1. **Demo Mode**: JSON 파일에서 샘플 데이터 로드 (AWS 액세스 불필요)\n2. **Live Mode**: AgentCore Observability에서 실제 trace 가져오기, 자체 ground truth 파일 제공\n\n**이 노트북은 두 가지 evaluator를 시연합니다:**\n- Output evaluation: 실제 응답을 예상 ground truth와 비교\n- Trajectory evaluation: 실제 tool 사용을 예상 tool과 비교\n\nStrands Evals는 거의 모든 평가 유형에 대한 커스텀 evaluator를 지원합니다—점수 기준으로 표현할 수 있는 모든 기준을 평가하도록 이 패턴을 확장할 수 있습니다.\n\n**워크플로우:**\n1. trace 로드 (demo 파일 또는 AgentCore Observability에서 실시간)\n2. ground truth 기대값 로드 (예상 출력/trajectory)\n3. trace_id로 병합\n4. 실제와 예상을 비교하는 evaluator 실행\n5. AgentCore Observability에 결과 로깅 (선택 사항)\n6. 결과 분석 및 격차 식별"
  },
  {
   "cell_type": "markdown",
   "source": "## 이 노트북의 위치\n\n이것은 **Notebook 3 (Option B)** - 예상 ground truth 출력과 비교하여 세션을 평가합니다.\n\n![Notebook Workflow](images/notebook_workflow.svg)\n\n## Ground Truth Evaluation 작동 방식\n\nSME(Subject Matter Expert)가 예상 출력이 포함된 ground truth 파일을 생성합니다. 이것은 `trace_id`로 실제 trace와 병합됩니다:\n\n![Ground Truth Flow](images/ground_truth_flow.svg)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "모듈을 import하고 로깅을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from strands_evals import Case, Experiment\n",
    "from strands_evals.evaluators import OutputEvaluator, TrajectoryEvaluator\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\nDemo Mode(샘플 파일 사용)와 Live Mode(AgentCore Observability에서 가져오기) 중 선택합니다.\n\n**Demo Mode** (`USE_DEMO_MODE = True`):\n- `DEMO_TRACES_PATH`에서 trace 로드\n- `DEMO_GROUND_TRUTH_PATH`에서 ground truth 로드\n- AWS 자격 증명 불필요\n\n**Live Mode** (`USE_DEMO_MODE = False`):\n- `SESSION_ID`를 사용하여 AgentCore Observability에서 trace 가져오기\n- `GROUND_TRUTH_PATH`에서 ground truth 로드 (이 파일을 직접 생성해야 함)\n- AWS 자격 증명 및 `config.py` 설정 필요\n\n**CloudWatch Logging** (`LOG_TO_CLOUDWATCH = True`):\n- AgentCore Observability 대시보드로 평가 결과 전송\n- `EVALUATION_CONFIG_ID` 및 evaluator 이름 필요"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Mode Selection\n",
    "# =============================================================================\n",
    "USE_DEMO_MODE = True\n",
    "\n",
    "# =============================================================================\n",
    "# Demo Mode Paths\n",
    "# =============================================================================\n",
    "DEMO_TRACES_PATH = \"demo_traces.json\"           # Actual agent responses\n",
    "DEMO_GROUND_TRUTH_PATH = \"demo_ground_truth.json\"  # Your expected outputs\n",
    "\n",
    "# =============================================================================\n",
    "# Live Mode Settings\n",
    "# =============================================================================\n",
    "SESSION_ID = \"your-session-id-here\"              # Session to evaluate\n",
    "GROUND_TRUTH_PATH = \"my_ground_truth.json\"       # Your ground truth file\n",
    "\n",
    "# =============================================================================\n",
    "# CloudWatch Logging\n",
    "# =============================================================================\n",
    "LOG_TO_CLOUDWATCH = True\n",
    "OUTPUT_EVALUATOR_NAME = \"Custom.GroundTruthOutput\"\n",
    "TRAJECTORY_EVALUATOR_NAME = \"Custom.GroundTruthTrajectory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 파일 형식\n\nGround truth 평가는 동일한 `session_id`를 공유하는 **두 개의 별도 파일**을 사용합니다:\n\n**주요 개념:**\n- `session_id`: 단일 사용자 세션의 모든 trace를 그룹화\n- `trace_id`: 세션 내 각 개별 상호작용을 식별\n\n### 1. Traces 파일 (실제 agent 응답)\nagent가 실제로 수행한 작업 포함 - CloudWatch에서 가져오거나 로컬에 저장:\n```json\n{\n  \"session_id\": \"5B467129-E54A-4F70-908D-CB31818004B5\",\n  \"traces\": [\n    {\n      \"trace_id\": \"693cb6c4e931\",\n      \"user_prompt\": \"What is the best route for a NZ road trip?\",\n      \"actual_output\": \"Based on the search results, here are the best routes...\",\n      \"actual_trajectory\": [\"web_search\"]\n    },\n    {\n      \"trace_id\": \"693cb6fa87aa\",\n      \"user_prompt\": \"Should I visit North or South Island?\",\n      \"actual_output\": \"Here's how the islands compare...\",\n      \"actual_trajectory\": [\"web_search\"]\n    }\n  ]\n}\n```\n\n### 2. Ground Truth 파일 (예상 출력)\nSME가 trace를 검토하고 각 `trace_id`에 대한 예상 출력을 작성:\n```json\n{\n  \"session_id\": \"5B467129-E54A-4F70-908D-CB31818004B5\",\n  \"ground_truth\": [\n    {\n      \"trace_id\": \"693cb6c4e931\",\n      \"user_prompt_reference\": \"What is the best route for a NZ road trip?\",\n      \"expected_output\": \"Response should mention Milford Road, Southern Scenic Route...\",\n      \"expected_trajectory\": [\"web_search\"]\n    },\n    {\n      \"trace_id\": \"693cb6fa87aa\",\n      \"user_prompt_reference\": \"Should I visit North or South Island?\",\n      \"expected_output\": \"Response should compare both islands with key features...\",\n      \"expected_trajectory\": [\"web_search\"]\n    }\n  ]\n}\n```\n\n**참고:** `user_prompt_reference`는 선택 사항입니다 - SME가 어떤 trace에 대한 기대값을 작성하는지 기억하는 데 도움이 됩니다."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace 및 Ground Truth 로드\n",
    "\n",
    "demo 파일 또는 CloudWatch에서 trace 데이터를 로드한 다음, ground truth 기대값을 로드하고 `trace_id`로 병합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DEMO_MODE:\n",
    "    # Load traces (actual agent responses)\n",
    "    with open(DEMO_TRACES_PATH, \"r\") as f:\n",
    "        traces_data = json.load(f)\n",
    "    \n",
    "    SESSION_ID = traces_data[\"session_id\"]\n",
    "    traces = []\n",
    "    for i, t in enumerate(traces_data[\"traces\"]):\n",
    "        traces.append({\n",
    "            \"trace_index\": i,\n",
    "            \"trace_id\": t.get(\"trace_id\", f\"demo-trace-{i:03d}\"),\n",
    "            \"user_prompt\": t[\"user_prompt\"],\n",
    "            \"actual_output\": t.get(\"actual_output\", \"\"),\n",
    "            \"actual_trajectory\": t.get(\"actual_trajectory\", []),\n",
    "        })\n",
    "    \n",
    "    # Load ground truth (expected outputs) - separate file!\n",
    "    with open(DEMO_GROUND_TRUTH_PATH, \"r\") as f:\n",
    "        gt_data = json.load(f)\n",
    "    \n",
    "    # Build ground truth lookup by trace_id\n",
    "    gt_by_trace_id = {\n",
    "        gt[\"trace_id\"]: {\n",
    "            \"expected_output\": gt[\"expected_output\"],\n",
    "            \"expected_trajectory\": gt.get(\"expected_trajectory\", []),\n",
    "        }\n",
    "        for gt in gt_data[\"ground_truth\"]\n",
    "    }\n",
    "    \n",
    "    # Merge: match traces to ground truth by trace_id\n",
    "    ground_truth = {}\n",
    "    matched_count = 0\n",
    "    for trace in traces:\n",
    "        trace_id = trace[\"trace_id\"]\n",
    "        if trace_id in gt_by_trace_id:\n",
    "            ground_truth[trace[\"trace_index\"]] = gt_by_trace_id[trace_id]\n",
    "            matched_count += 1\n",
    "    \n",
    "    print(f\"Demo Mode:\")\n",
    "    print(f\"  Traces loaded: {len(traces)} from {DEMO_TRACES_PATH}\")\n",
    "    print(f\"  Ground truth loaded: {len(gt_data['ground_truth'])} entries from {DEMO_GROUND_TRUTH_PATH}\")\n",
    "    print(f\"  Matched by trace_id: {matched_count}\")\n",
    "    print(f\"  Session ID: {SESSION_ID}\")\n",
    "    if gt_data.get(\"description\"):\n",
    "        print(f\"  Description: {gt_data['description']}\")\n",
    "\n",
    "else:\n",
    "    from config import AWS_REGION, SOURCE_LOG_GROUP, LOOKBACK_HOURS\n",
    "    from utils import CloudWatchSessionMapper, ObservabilityClient\n",
    "    from strands_evals.types.trace import AgentInvocationSpan, ToolExecutionSpan\n",
    "    \n",
    "    # Fetch traces from CloudWatch\n",
    "    obs_client = ObservabilityClient(region_name=AWS_REGION, log_group=SOURCE_LOG_GROUP)\n",
    "    mapper = CloudWatchSessionMapper()\n",
    "    \n",
    "    end_time = datetime.now(timezone.utc)\n",
    "    start_time = end_time - timedelta(hours=LOOKBACK_HOURS)\n",
    "    \n",
    "    trace_data = obs_client.get_session_data(\n",
    "        session_id=SESSION_ID,\n",
    "        start_time_ms=int(start_time.timestamp() * 1000),\n",
    "        end_time_ms=int(end_time.timestamp() * 1000),\n",
    "        include_runtime_logs=False,\n",
    "    )\n",
    "    \n",
    "    if not trace_data.spans:\n",
    "        raise ValueError(f\"No spans found for session {SESSION_ID}\")\n",
    "    \n",
    "    session = trace_data.to_session(mapper)\n",
    "    traces = []\n",
    "    for i, trace in enumerate(session.traces):\n",
    "        agent_span = None\n",
    "        tool_calls = []\n",
    "        for span in trace.spans:\n",
    "            if isinstance(span, AgentInvocationSpan):\n",
    "                agent_span = span\n",
    "            elif isinstance(span, ToolExecutionSpan):\n",
    "                tool_calls.append(span.tool_call.name)\n",
    "        if agent_span:\n",
    "            traces.append({\n",
    "                \"trace_index\": i,\n",
    "                \"trace_id\": trace.trace_id,\n",
    "                \"user_prompt\": agent_span.user_prompt or \"\",\n",
    "                \"actual_output\": agent_span.agent_response or \"\",\n",
    "                \"actual_trajectory\": tool_calls,\n",
    "            })\n",
    "    \n",
    "    # Load ground truth from separate file\n",
    "    try:\n",
    "        with open(GROUND_TRUTH_PATH, \"r\") as f:\n",
    "            gt_data = json.load(f)\n",
    "        gt_by_trace_id = {\n",
    "            gt[\"trace_id\"]: {\n",
    "                \"expected_output\": gt[\"expected_output\"],\n",
    "                \"expected_trajectory\": gt.get(\"expected_trajectory\", []),\n",
    "            }\n",
    "            for gt in gt_data[\"ground_truth\"]\n",
    "        }\n",
    "        ground_truth = {}\n",
    "        for trace in traces:\n",
    "            trace_id = trace[\"trace_id\"]\n",
    "            if trace_id in gt_by_trace_id:\n",
    "                ground_truth[trace[\"trace_index\"]] = gt_by_trace_id[trace_id]\n",
    "        print(f\"Ground truth loaded: {len(ground_truth)} matches from {GROUND_TRUTH_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        ground_truth = {}\n",
    "        print(f\"Ground truth file not found: {GROUND_TRUTH_PATH}\")\n",
    "        print(\"Create a ground truth file or define manually in the next section\")\n",
    "    \n",
    "    print(f\"Live Mode: Loaded {len(traces)} traces from CloudWatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace 검토\n",
    "\n",
    "각 trace의 세부 정보를 표시합니다. 각 상호작용에 대한 ground truth가 어떻게 보여야 하는지 이해하기 위해 이를 검토하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace in traces:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRACE {trace['trace_index'] + 1} (ID: {trace['trace_id']})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    prompt = trace['user_prompt']\n",
    "    output = trace['actual_output']\n",
    "    \n",
    "    print(f\"\\nUSER PROMPT:\\n{prompt[:500]}...\" if len(prompt) > 500 else f\"\\nUSER PROMPT:\\n{prompt}\")\n",
    "    print(f\"\\nACTUAL OUTPUT:\\n{output[:500]}...\" if len(output) > 500 else f\"\\nACTUAL OUTPUT:\\n{output}\")\n",
    "    print(f\"\\nACTUAL TRAJECTORY: {trace['actual_trajectory']}\")\n",
    "    \n",
    "    if trace['trace_index'] in ground_truth:\n",
    "        gt = ground_truth[trace['trace_index']]\n",
    "        print(f\"\\nEXPECTED OUTPUT: {gt['expected_output']}\")\n",
    "        print(f\"EXPECTED TRAJECTORY: {gt['expected_trajectory']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth 정의 (Live Mode 전용)\n",
    "\n",
    "Live Mode를 사용하는 경우, 여기에서 각 trace에 대한 예상 출력 및 예상 trajectory를 정의합니다.\n",
    "\n",
    "**Demo Mode에서는 ground truth가 JSON 파일에서 미리 로드됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ground_truth:\n",
    "    print(\"Ground truth not defined. Generating template...\\n\")\n",
    "    print(\"Copy, edit, and paste the following:\\n\")\n",
    "    print(\"ground_truth = {\")\n",
    "    for trace in traces:\n",
    "        prompt_preview = trace['user_prompt'][:50].replace('\"', \"'\")\n",
    "        print(f\"    {trace['trace_index']}: {{\")\n",
    "        print(f'        \"expected_output\": \"TODO: {prompt_preview}...\",') \n",
    "        print(f'        \"expected_trajectory\": {trace[\"actual_trajectory\"]},')\n",
    "        print(f\"    }},\")\n",
    "    print(\"}\")\n",
    "else:\n",
    "    print(f\"Ground truth already defined for {len(ground_truth)} traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Rubric\n",
    "\n",
    "두 evaluator 모두 rubric을 사용하여 실제 값과 예상 값을 비교합니다:\n",
    "\n",
    "- **OutputEvaluator**: 의미적 유사성을 사용하여 실제 응답을 예상 출력과 비교\n",
    "- **TrajectoryEvaluator**: 실제 tool trajectory를 예상 trajectory와 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_output_rubric = \"\"\"\n",
    "Compare the agent's actual output against the expected ground truth output.\n",
    "\n",
    "Evaluation criteria:\n",
    "1. Semantic Match (0-0.5): Does the actual output convey the same meaning as expected?\n",
    "   - 0.5: Full semantic alignment - same information and intent\n",
    "   - 0.3: Partial alignment - captures main points but misses details\n",
    "   - 0.0: No alignment - different information or wrong answer\n",
    "\n",
    "2. Completeness (0-0.3): Does the actual output include all key points from expected?\n",
    "   - 0.3: All key information present\n",
    "   - 0.15: Most key information present\n",
    "   - 0.0: Missing critical information\n",
    "\n",
    "3. Correctness (0-0.2): Is the actual output factually consistent with expected?\n",
    "   - 0.2: No factual contradictions\n",
    "   - 0.1: Minor inconsistencies\n",
    "   - 0.0: Major contradictions or errors\n",
    "\n",
    "Final score = sum of all criteria (0.0 to 1.0)\n",
    "\"\"\"\n",
    "\n",
    "trajectory_rubric = \"\"\"\n",
    "Compare the agent's actual tool trajectory against the expected trajectory.\n",
    "\n",
    "Evaluation criteria:\n",
    "1. Tool Match (0-0.5): Did the agent use the expected tools?\n",
    "   - 0.5: All expected tools were used\n",
    "   - 0.25: Some expected tools were used\n",
    "   - 0.0: None of the expected tools were used\n",
    "\n",
    "2. No Extra Tools (0-0.3): Did the agent avoid unnecessary tools?\n",
    "   - 0.3: No extra tools beyond expected\n",
    "   - 0.15: One extra tool\n",
    "   - 0.0: Multiple unnecessary tools\n",
    "\n",
    "3. Order (0-0.2): Were tools used in the expected sequence?\n",
    "   - 0.2: Correct order\n",
    "   - 0.1: Minor order differences\n",
    "   - 0.0: Completely different order\n",
    "\n",
    "Final score = sum of all criteria (0.0 to 1.0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Case 생성\n",
    "\n",
    "비교를 위해 실제 출력과 예상 ground truth를 모두 포함하는 Case를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_cases(traces: List[Dict], ground_truth: Dict[int, Dict], session_id: str) -> List[Case]:\n",
    "    \"\"\"Create evaluation cases with ground truth for comparison.\"\"\"\n",
    "    cases = []\n",
    "    \n",
    "    for trace in traces:\n",
    "        idx = trace[\"trace_index\"]\n",
    "        gt = ground_truth.get(idx, {})\n",
    "        \n",
    "        if not gt:\n",
    "            print(f\"Warning: No ground truth for trace {idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        case = Case(\n",
    "            name=f\"trace_{idx}_{trace['trace_id'][:8]}\",\n",
    "            input=trace[\"user_prompt\"],\n",
    "            expected_output=gt.get(\"expected_output\", \"\"),\n",
    "            session_id=session_id,\n",
    "            metadata={\n",
    "                \"actual_output\": trace[\"actual_output\"],\n",
    "                \"actual_trajectory\": trace[\"actual_trajectory\"],\n",
    "                \"expected_trajectory\": gt.get(\"expected_trajectory\", []),\n",
    "                \"trace_id\": trace[\"trace_id\"],\n",
    "            },\n",
    "        )\n",
    "        cases.append(case)\n",
    "    \n",
    "    return cases\n",
    "\n",
    "cases = create_ground_truth_cases(traces, ground_truth, SESSION_ID)\n",
    "print(f\"Created {len(cases)} evaluation cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 함수\n",
    "\n",
    "이 함수들은 evaluator가 비교할 실제 값과 예상 값을 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_task_fn(case: Case) -> str:\n",
    "    \"\"\"Return actual and expected output for comparison.\"\"\"\n",
    "    actual = case.metadata.get(\"actual_output\", \"\")\n",
    "    expected = case.expected_output or \"\"\n",
    "    return f\"ACTUAL OUTPUT:\\n{actual}\\n\\nEXPECTED OUTPUT (Ground Truth):\\n{expected}\"\n",
    "\n",
    "\n",
    "def trajectory_task_fn(case: Case):\n",
    "    \"\"\"Return output and trajectory for TrajectoryEvaluator.\n",
    "    \n",
    "    TrajectoryEvaluator expects a dictionary with 'output' and 'trajectory' keys.\n",
    "    \"\"\"\n",
    "    actual_output = case.metadata.get(\"actual_output\", \"\")\n",
    "    actual_trajectory = case.metadata.get(\"actual_trajectory\", [])\n",
    "    expected_trajectory = case.metadata.get(\"expected_trajectory\", [])\n",
    "    \n",
    "    # Format output to include comparison context\n",
    "    comparison_output = f\"\"\"ACTUAL OUTPUT:\n",
    "{actual_output}\n",
    "\n",
    "EXPECTED TRAJECTORY: {expected_trajectory}\n",
    "ACTUAL TRAJECTORY: {actual_trajectory}\"\"\"\n",
    "    \n",
    "    # Return dictionary format expected by TrajectoryEvaluator\n",
    "    return {\"output\": comparison_output, \"trajectory\": actual_trajectory}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Evaluation 실행\n",
    "\n",
    "실제 출력을 ground truth와 비교하는 평가를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"Running Output Evaluation...\")\n",
    "    output_evaluator = OutputEvaluator(rubric=ground_truth_output_rubric)\n",
    "    output_experiment = Experiment(cases=cases, evaluators=[output_evaluator])\n",
    "    output_results = output_experiment.run_evaluations(ground_truth_task_fn)\n",
    "    output_report = output_results[0]  # Extract the report from the list\n",
    "    print(f\"Output Evaluation Complete - Overall Score: {output_report.overall_score:.2f}\")\n",
    "else:\n",
    "    print(\"No cases to evaluate. Please define ground truth first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"Running Trajectory Evaluation...\")\n",
    "    \n",
    "    # Collect all unique tools from actual and expected trajectories\n",
    "    all_tools = set()\n",
    "    for trace in traces:\n",
    "        all_tools.update(trace[\"actual_trajectory\"])\n",
    "    for gt in ground_truth.values():\n",
    "        all_tools.update(gt.get(\"expected_trajectory\", []))\n",
    "    \n",
    "    trajectory_evaluator = TrajectoryEvaluator(\n",
    "        rubric=trajectory_rubric,\n",
    "        trajectory_description={\"available_tools\": list(all_tools)}\n",
    "    )\n",
    "    trajectory_experiment = Experiment(cases=cases, evaluators=[trajectory_evaluator])\n",
    "    trajectory_results = trajectory_experiment.run_evaluations(trajectory_task_fn)\n",
    "    trajectory_report = trajectory_results[0]\n",
    "    print(f\"Trajectory Evaluation Complete - Overall Score: {trajectory_report.overall_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상세 결과\n",
    "\n",
    "trace별 점수 및 설명을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GROUND TRUTH EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, case in enumerate(cases):\n",
    "        print(f\"\\n--- Trace {i+1}: {case.name} ---\")\n",
    "        prompt_display = case.input[:80] + \"...\" if len(case.input) > 80 else case.input\n",
    "        print(f\"User Prompt: {prompt_display}\")\n",
    "        \n",
    "        output_score = output_report.scores[i] if i < len(output_report.scores) else 0\n",
    "        output_reason = output_report.reasons[i] if i < len(output_report.reasons) else \"N/A\"\n",
    "        print(f\"\\nOutput Score: {output_score:.2f}\")\n",
    "        print(f\"Explanation: {output_reason[:250]}...\" if len(str(output_reason)) > 250 else f\"Explanation: {output_reason}\")\n",
    "        \n",
    "        traj_score = trajectory_report.scores[i] if i < len(trajectory_report.scores) else 0\n",
    "        traj_reason = trajectory_report.reasons[i] if i < len(trajectory_report.reasons) else \"N/A\"\n",
    "        print(f\"\\nTrajectory Score: {traj_score:.2f}\")\n",
    "        print(f\"Expected Tools: {case.metadata.get('expected_trajectory', [])}\")\n",
    "        print(f\"Actual Tools:   {case.metadata.get('actual_trajectory', [])}\")\n",
    "        print(f\"Explanation: {traj_reason[:250]}...\" if len(str(traj_reason)) > 250 else f\"Explanation: {traj_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## CloudWatch에 결과 로깅 (선택 사항)\n\n원본 trace ID를 사용하여 AgentCore Observability 대시보드로 평가 결과를 전송합니다. 이를 통해 AgentCore Observability 콘솔에서 원본 trace와 함께 ground truth 평가 점수를 볼 수 있습니다.\n\n이 기능을 활성화하려면 configuration 섹션에서 `LOG_TO_CLOUDWATCH = True`로 설정하세요."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_CLOUDWATCH and cases:\n",
    "    from config import EVALUATION_CONFIG_ID, setup_cloudwatch_environment\n",
    "    from utils import send_evaluation_to_cloudwatch\n",
    "    \n",
    "    # Setup CloudWatch environment\n",
    "    setup_cloudwatch_environment()\n",
    "    \n",
    "    output_logged = 0\n",
    "    trajectory_logged = 0\n",
    "    \n",
    "    print(\"Logging results to CloudWatch...\")\n",
    "    \n",
    "    for i, case in enumerate(cases):\n",
    "        trace_id = case.metadata.get(\"trace_id\", \"\")\n",
    "        if not trace_id:\n",
    "            continue\n",
    "        \n",
    "        # Log output evaluation result\n",
    "        output_score = output_report.scores[i] if i < len(output_report.scores) else 0\n",
    "        output_reason = output_report.reasons[i] if i < len(output_report.reasons) else \"\"\n",
    "        if send_evaluation_to_cloudwatch(\n",
    "            trace_id=trace_id,\n",
    "            session_id=SESSION_ID,\n",
    "            evaluator_name=OUTPUT_EVALUATOR_NAME,\n",
    "            score=output_score,\n",
    "            explanation=str(output_reason)[:500],\n",
    "            config_id=EVALUATION_CONFIG_ID,\n",
    "        ):\n",
    "            output_logged += 1\n",
    "        \n",
    "        # Log trajectory evaluation result\n",
    "        traj_score = trajectory_report.scores[i] if i < len(trajectory_report.scores) else 0\n",
    "        traj_reason = trajectory_report.reasons[i] if i < len(trajectory_report.reasons) else \"\"\n",
    "        if send_evaluation_to_cloudwatch(\n",
    "            trace_id=trace_id,\n",
    "            session_id=SESSION_ID,\n",
    "            evaluator_name=TRAJECTORY_EVALUATOR_NAME,\n",
    "            score=traj_score,\n",
    "            explanation=str(traj_reason)[:500],\n",
    "            config_id=EVALUATION_CONFIG_ID,\n",
    "        ):\n",
    "            trajectory_logged += 1\n",
    "    \n",
    "    print(f\"CloudWatch logging complete:\")\n",
    "    print(f\"  Output evaluations logged: {output_logged}/{len(cases)}\")\n",
    "    print(f\"  Trajectory evaluations logged: {trajectory_logged}/{len(cases)}\")\n",
    "else:\n",
    "    if not LOG_TO_CLOUDWATCH:\n",
    "        print(\"CloudWatch logging disabled. Set LOG_TO_CLOUDWATCH = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "agent가 ground truth와 얼마나 일치하는지 보여주는 집계 결과입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSession: {SESSION_ID}\")\n",
    "    print(f\"Mode: {'Demo' if USE_DEMO_MODE else 'Live'}\")\n",
    "    print(f\"Traces Evaluated: {len(cases)}\")\n",
    "    \n",
    "    print(f\"\\nOutput Evaluation (Actual vs Expected Response):\")\n",
    "    print(f\"  Overall Score: {output_report.overall_score:.2f}\")\n",
    "    print(f\"  Range: {min(output_report.scores):.2f} - {max(output_report.scores):.2f}\")\n",
    "    \n",
    "    print(f\"\\nTrajectory Evaluation (Actual vs Expected Tools):\")\n",
    "    print(f\"  Overall Score: {trajectory_report.overall_score:.2f}\")\n",
    "    print(f\"  Range: {min(trajectory_report.scores):.2f} - {max(trajectory_report.scores):.2f}\")\n",
    "    \n",
    "    low_output = [i+1 for i, s in enumerate(output_report.scores) if s < 0.5]\n",
    "    low_traj = [i+1 for i, s in enumerate(trajectory_report.scores) if s < 0.5]\n",
    "    \n",
    "    if low_output:\n",
    "        print(f\"\\nTraces with low output scores (<0.5): {low_output}\")\n",
    "    if low_traj:\n",
    "        print(f\"Traces with low trajectory scores (<0.5): {low_traj}\")\n",
    "    \n",
    "    if not low_output and not low_traj:\n",
    "        print(f\"\\nAll traces scored above 0.5 - agent behavior matches ground truth well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 내보내기\n",
    "\n",
    "평가 결과를 JSON으로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    export_data = {\n",
    "        \"evaluation_time\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"session_id\": SESSION_ID,\n",
    "        \"mode\": \"demo\" if USE_DEMO_MODE else \"live\",\n",
    "        \"evaluation_type\": \"ground_truth\",\n",
    "        \"summary\": {\n",
    "            \"traces_evaluated\": len(cases),\n",
    "            \"output_overall_score\": output_report.overall_score,\n",
    "            \"trajectory_overall_score\": trajectory_report.overall_score,\n",
    "        },\n",
    "        \"traces\": [\n",
    "            {\n",
    "                \"trace_id\": case.metadata.get(\"trace_id\"),\n",
    "                \"user_prompt\": case.input,\n",
    "                \"expected_output\": case.expected_output,\n",
    "                \"actual_output\": case.metadata.get(\"actual_output\"),\n",
    "                \"expected_trajectory\": case.metadata.get(\"expected_trajectory\"),\n",
    "                \"actual_trajectory\": case.metadata.get(\"actual_trajectory\"),\n",
    "                \"output_score\": output_report.scores[i] if i < len(output_report.scores) else None,\n",
    "                \"trajectory_score\": trajectory_report.scores[i] if i < len(trajectory_report.scores) else None,\n",
    "            }\n",
    "            for i, case in enumerate(cases)\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    output_path = \"ground_truth_results.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Results exported to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}