{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 멀티 세션 Evaluation\n\n이 노트북은 Strands Evals를 사용하여 agent 세션을 평가합니다. Strands Evals는 LLM을 심사자로 사용하는 확장 가능한 LLM 기반 evaluation 프레임워크입니다. 각 세션에 대해 AgentCore Observability에서 trace를 가져오고, evaluator를 실행하며, 대시보드 상관관계를 위해 원본 trace ID와 함께 결과를 다시 로깅합니다.\n\n**이 노트북은 두 가지 evaluator를 시연합니다:**\n- **OutputEvaluator**: 응답 품질 점수 (관련성, 정확성, 완전성)\n- **TrajectoryEvaluator**: Tool 사용 점수 (선택, 효율성, 순서)\n\nStrands Evals는 거의 모든 evaluation 유형에 대한 커스텀 evaluator를 지원합니다. 프레임워크의 강력함은 rubric 시스템에 있습니다—기준을 정의하면 LLM이 일관되게 적용합니다.\n\n**워크플로우:**\n1. discovery 노트북에서 세션 로드 (또는 커스텀 세션 ID 제공)\n2. 각 세션에 대해: trace 가져오기, evaluation case 생성, evaluator 실행\n3. EMF 형식으로 AgentCore에 결과 로깅\n4. 요약 통계 생성\n\n**전제 조건:** 먼저 세션 discovery 노트북을 실행하거나 세션 ID 목록을 준비하세요."
  },
  {
   "cell_type": "markdown",
   "source": "## 이 노트북의 위치\n\n이것은 **Notebook 2 (Option A)** - 사용자가 정의한 커스텀 rubric을 사용하여 세션을 평가합니다.\n\n![Notebook Workflow](images/notebook_workflow.svg)\n\n## 데이터 흐름\n\nEvaluation 파이프라인은 AgentCore Observability trace를 점수화된 결과로 변환합니다:\n\n![Evaluation Pipeline](images/evaluation_pipeline.svg)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 설정\n\nStrands Evals evaluator와 AgentCore Observability 상호작용을 위한 유틸리티 클래스를 포함한 필수 모듈을 가져옵니다. 구성은 `config.py`에서 로드됩니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List\n",
    "\n",
    "# 현재 디렉토리를 path에 추가하여 로컬 모듈 import 가능하게 설정\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from config import (\n",
    "    AWS_REGION,\n",
    "    AWS_ACCOUNT_ID,\n",
    "    SOURCE_LOG_GROUP,\n",
    "    EVAL_RESULTS_LOG_GROUP,\n",
    "    LOOKBACK_HOURS,\n",
    "    MAX_CASES_PER_SESSION,\n",
    "    DISCOVERED_SESSIONS_PATH,\n",
    "    RESULTS_JSON_PATH,\n",
    "    EVALUATION_CONFIG_ID,\n",
    "    setup_cloudwatch_environment,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    CloudWatchSessionMapper,  # CloudWatch span을 Strands Eval 형식으로 변환\n",
    "    ObservabilityClient,  # AgentCore Observability에서 trace 데이터 가져오기\n",
    "    SessionDiscoveryResult,\n",
    "    SessionInfo,\n",
    "    send_evaluation_to_cloudwatch,  # Evaluation 결과를 CloudWatch에 로깅\n",
    ")\n",
    "\n",
    "# Strands Evals: LLM 기반 evaluation 프레임워크\n",
    "from strands_evals import Case, Experiment\n",
    "from strands_evals.evaluators import OutputEvaluator, TrajectoryEvaluator\n",
    "from strands_evals.types.trace import AgentInvocationSpan, ToolExecutionSpan\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 구성\n\nCloudWatch 메트릭을 위한 evaluator 이름을 정의합니다. 이 이름들은 AgentCore Observability 대시보드에 표시되며 `Custom.YourEvaluatorName` 규칙을 따라야 합니다. `EVALUATION_CONFIG_ID`는 `config.py`에서 로드됩니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CloudWatch 메트릭에 표시될 evaluator 이름 (Custom. prefix 필수)\n",
    "OUTPUT_EVALUATOR_NAME = \"Custom.OutputEvaluator\"  # 응답 품질 평가\n",
    "TRAJECTORY_EVALUATOR_NAME = \"Custom.TrajectoryEvaluator\"  # Tool 사용 패턴 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudWatch 환경\n",
    "\n",
    "Evaluation 결과 로깅에 필요한 환경 변수를 구성합니다. OTEL 리소스 속성에 대해 `config.py`의 `SERVICE_NAME`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_cloudwatch_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세션 로드\n",
    "\n",
    "Discovery 노트북 JSON 출력에서 세션을 로드합니다. 또는 `USE_JSON_FILE = False`로 설정하고 특정 세션의 타겟 재평가를 위해 커스텀 세션 ID를 직접 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to False to provide custom session IDs instead\n",
    "USE_JSON_FILE = True\n",
    "\n",
    "if USE_JSON_FILE:\n",
    "    discovery_result = SessionDiscoveryResult.load_from_json(DISCOVERED_SESSIONS_PATH)\n",
    "    sessions_to_process = discovery_result.sessions\n",
    "else:\n",
    "    # Provide custom session IDs here\n",
    "    session_ids = [\n",
    "        \"your-session-id-here\",\n",
    "    ]\n",
    "    sessions_to_process = [\n",
    "        SessionInfo(\n",
    "            session_id=sid,\n",
    "            span_count=0,\n",
    "            first_seen=datetime.now(timezone.utc),\n",
    "            last_seen=datetime.now(timezone.utc),\n",
    "            discovery_method=\"user_provided\",\n",
    "        )\n",
    "        for sid in session_ids\n",
    "    ]\n",
    "\n",
    "print(f\"Loaded {len(sessions_to_process)} sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Rubric\n",
    "\n",
    "Rubric은 evaluation 기준을 정의합니다. Evaluator는 rubric과 agent의 출력을 LLM에 전송하고, LLM은 심사자 역할을 하여 설명과 함께 점수(0.0-1.0)를 반환합니다.\n",
    "\n",
    "**효과적인 rubric 작성:**\n",
    "- 좋은 품질과 나쁜 품질을 구성하는 요소에 대해 구체적으로 작성\n",
    "- 점수 기준점 포함 (1.0 vs 0.5 vs 0.0은 무엇을 의미하는가?)\n",
    "- Agent의 도메인과 관련된 측정 가능한 기준에 집중\n",
    "\n",
    "아래에서 이러한 rubric을 커스터마이즈하세요. 기본 rubric은 일반적인 응답 품질과 tool 사용 패턴을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rubric = \"\"\"\n",
    "Evaluate the agent's response based on:\n",
    "1. Relevance: Does the response directly address the user's question?\n",
    "2. Accuracy: Is the information factually correct?\n",
    "3. Completeness: Does the response provide sufficient detail?\n",
    "\n",
    "Score 0.0-1.0: 1.0=excellent, 0.5=adequate, 0.0=poor\n",
    "\"\"\"\n",
    "\n",
    "trajectory_rubric = \"\"\"\n",
    "Evaluate the agent's tool usage based on:\n",
    "1. Tool Selection: Did the agent choose appropriate tools?\n",
    "2. Efficiency: Were tools used without unnecessary calls?\n",
    "3. Logical Sequence: Were tools used in a logical order?\n",
    "\n",
    "Score 0.0-1.0: 1.0=optimal, 0.5=acceptable, 0.0=poor\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 헬퍼 함수\n\n이 함수들은 AgentCore Observability trace와 Strands Evals를 연결합니다:\n\n- `task_fn(case)`: OutputEvaluator가 rubric에 대해 점수를 매기기 위해 agent의 실제 응답을 반환합니다.\n\n- `trajectory_task_fn(case)`: TrajectoryEvaluator가 tool 사용 패턴을 평가하기 위해 응답과 tool 시퀀스를 모두 반환합니다.\n\n- `create_cases_from_session(session)`: Strands Eval Session을 evaluation Case로 변환합니다. AgentInvocationSpan에서 사용자 프롬프트를 추출하고, ToolExecutionSpan 객체에서 tool 이름을 추출하며, CloudWatch 상관관계를 위해 원본 trace_id를 보존합니다.\n\n- `log_case_result_to_cloudwatch(case, ...)`: 원본 trace_id를 사용하여 AgentCore Observability에 evaluation 결과를 전송하여 대시보드에서 원본 trace와 함께 점수를 볼 수 있도록 합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_fn(case: Case) -> str:\n",
    "    \"\"\"Return actual output from trace metadata.\"\"\"\n",
    "    return (case.metadata.get(\"actual_output\", \"\"))\n",
    "\n",
    "\n",
    "def trajectory_task_fn(case: Case):\n",
    "    \"\"\"Return output and trajectory from trace metadata.\"\"\"\n",
    "    return {\"output\": case.metadata.get(\"actual_output\", \"\"), \"trajectory\": case.metadata.get(\"trajectory_for_eval\", [])}\n",
    "\n",
    "def log_case_result_to_cloudwatch(case: Case, evaluator_name: str, score: float, explanation: str, label: str = None) -> bool:\n",
    "    \"\"\"Log evaluation result to CloudWatch with original trace ID.\"\"\"\n",
    "    trace_id = case.metadata.get(\"trace_id\", \"\")\n",
    "    if not trace_id:\n",
    "        return False\n",
    "    return send_evaluation_to_cloudwatch(\n",
    "        trace_id=trace_id,\n",
    "        session_id=case.session_id,\n",
    "        evaluator_name=evaluator_name,\n",
    "        score=score,\n",
    "        explanation=explanation,\n",
    "        label=label,\n",
    "        config_id=EVALUATION_CONFIG_ID,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_cases_from_session(session, session_id: str, max_cases: int = None) -> List[Case]:\n",
    "    \"\"\"Create evaluation cases from a Strands Eval Session.\"\"\"\n",
    "    cases = []\n",
    "    for i, trace in enumerate(session.traces):\n",
    "        if max_cases and len(cases) >= max_cases:\n",
    "            break\n",
    "        agent_span = None\n",
    "        tool_names = []\n",
    "        for span in trace.spans:\n",
    "            if isinstance(span, AgentInvocationSpan):\n",
    "                agent_span = span\n",
    "            elif isinstance(span, ToolExecutionSpan):\n",
    "                tool_names.append(span.tool_call.name)\n",
    "        if agent_span:\n",
    "            case = Case(\n",
    "                name=f\"trace_{i+1}_{trace.trace_id[:8]}\",\n",
    "                input=agent_span.user_prompt or \"\",\n",
    "                expected_output=\"\",\n",
    "                session_id=session_id,\n",
    "                metadata={\n",
    "                    \"actual_output\": agent_span.agent_response or \"\",\n",
    "                    \"actual_trajectory\": tool_names,\n",
    "                    \"trace_id\": trace.trace_id,\n",
    "                    \"tool_count\": len(tool_names),\n",
    "                },\n",
    "            )\n",
    "            cases.append(case)\n",
    "    return cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 클라이언트 초기화\n\nTrace를 가져오기 위한 `ObservabilityClient`와 변환을 위한 `CloudWatchSessionMapper`를 생성합니다.\n\nMapper는 원시 AgentCore Observability span을 구조화된 Strands Eval 객체로 변환합니다:\n- 각 상호작용을 재구성하기 위해 trace_id별로 span을 그룹화\n- Tool 호출을 추출하고 결과와 매칭\n- 사용자 프롬프트(첫 번째 메시지)와 agent 응답(최종 출력) 식별\n- AgentInvocationSpan(전체 상호작용)과 ToolExecutionSpan(각 tool 사용)을 생성"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_client = ObservabilityClient(\n",
    "    region_name=AWS_REGION,\n",
    "    log_group=SOURCE_LOG_GROUP,\n",
    ")\n",
    "mapper = CloudWatchSessionMapper()\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(hours=LOOKBACK_HOURS)\n",
    "start_time_ms = int(start_time.timestamp() * 1000)\n",
    "end_time_ms = int(end_time.timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 세션 처리\n\n메인 evaluation 루프입니다. 각 세션에 대해:\n1. AgentCore Observability에서 span 가져오기\n2. Mapper를 사용하여 span을 Strands Eval Session 형식으로 변환\n3. 세션의 각 trace에서 evaluation Case 생성\n4. 모든 case에 대해 OutputEvaluator 실행\n5. Tool을 사용한 case에 대해 TrajectoryEvaluator 실행\n6. 대시보드 상관관계를 위해 원본 trace ID와 함께 모든 결과를 AgentCore Observability에 로깅\n\n각 세션에 대한 진행 상황이 출력됩니다. 오류는 캡처되고 로깅되며 루프를 중단하지 않습니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_session_results = []\n",
    "total_cases_evaluated = 0\n",
    "total_logs_sent = 0\n",
    "all_tools_used = set()\n",
    "\n",
    "for session_idx, session_info in enumerate(sessions_to_process):\n",
    "    session_id = session_info.session_id\n",
    "    print(f\"[{session_idx + 1}/{len(sessions_to_process)}] {session_id}\")\n",
    "\n",
    "    try:\n",
    "        trace_data = obs_client.get_session_data(\n",
    "            session_id=session_id,\n",
    "            start_time_ms=start_time_ms,\n",
    "            end_time_ms=end_time_ms,\n",
    "            include_runtime_logs=False,\n",
    "        )\n",
    "\n",
    "        if not trace_data.spans:\n",
    "            all_session_results.append({\"session_id\": session_id, \"status\": \"skipped\", \"reason\": \"no_spans\"})\n",
    "            continue\n",
    "\n",
    "        session = trace_data.to_session(mapper)\n",
    "        cases = create_cases_from_session(session, session_id, MAX_CASES_PER_SESSION)\n",
    "\n",
    "        if not cases:\n",
    "            all_session_results.append({\"session_id\": session_id, \"status\": \"skipped\", \"reason\": \"no_cases\"})\n",
    "            continue\n",
    "\n",
    "        for case in cases:\n",
    "            for tool in case.metadata.get(\"actual_trajectory\", []):\n",
    "                all_tools_used.add(tool)\n",
    "\n",
    "        # Run Output Evaluator\n",
    "        output_experiment = Experiment(cases=cases, evaluators=[OutputEvaluator(rubric=output_rubric)])\n",
    "        output_results = output_experiment.run_evaluations(task_fn)\n",
    "        output_report = output_results[0]\n",
    "\n",
    "        output_logged = 0\n",
    "        for i, case in enumerate(cases):\n",
    "            if log_case_result_to_cloudwatch(case, OUTPUT_EVALUATOR_NAME, output_report.scores[i], output_report.reasons[i] if i < len(output_report.reasons) else \"\"):\n",
    "                output_logged += 1\n",
    "\n",
    "        # Run Trajectory Evaluator\n",
    "        trajectory_cases = [c for c in cases if c.metadata.get(\"actual_trajectory\")]\n",
    "        trajectory_score = None\n",
    "        trajectory_logged = 0\n",
    "\n",
    "        if trajectory_cases:\n",
    "            traj_eval_cases = [\n",
    "                Case(name=c.name, input=c.input, expected_output=c.expected_output, session_id=c.session_id,\n",
    "                     metadata={**c.metadata, \"trajectory_for_eval\": c.metadata.get(\"actual_trajectory\", [])})\n",
    "                for c in trajectory_cases\n",
    "            ]\n",
    "            trajectory_experiment = Experiment(\n",
    "                cases=traj_eval_cases,\n",
    "                evaluators=[TrajectoryEvaluator(rubric=trajectory_rubric, trajectory_description={\"available_tools\": list(all_tools_used)})]\n",
    "            )\n",
    "            trajectory_results = trajectory_experiment.run_evaluations(trajectory_task_fn)\n",
    "            trajectory_report = trajectory_results[0]\n",
    "            trajectory_score = trajectory_report.overall_score\n",
    "\n",
    "            for i, case in enumerate(traj_eval_cases):\n",
    "                if log_case_result_to_cloudwatch(case, TRAJECTORY_EVALUATOR_NAME, trajectory_report.scores[i], trajectory_report.reasons[i] if i < len(trajectory_report.reasons) else \"\"):\n",
    "                    trajectory_logged += 1\n",
    "\n",
    "        all_session_results.append({\n",
    "            \"session_id\": session_id,\n",
    "            \"status\": \"completed\",\n",
    "            \"case_count\": len(cases),\n",
    "            \"output_score\": output_report.overall_score,\n",
    "            \"trajectory_score\": trajectory_score,\n",
    "            \"logs_sent\": output_logged + trajectory_logged,\n",
    "        })\n",
    "        total_cases_evaluated += len(cases)\n",
    "        total_logs_sent += output_logged + trajectory_logged\n",
    "\n",
    "    except Exception as e:\n",
    "        all_session_results.append({\"session_id\": session_id, \"status\": \"error\", \"error\": str(e)})\n",
    "\n",
    "print(f\"\\nCompleted: {len([r for r in all_session_results if r['status'] == 'completed'])} sessions, {total_cases_evaluated} cases, {total_logs_sent} logs sent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "완료율, 평가된 총 case 수, output 및 trajectory evaluator의 평균 점수를 포함하여 평가된 모든 세션의 집계 통계입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = [r for r in all_session_results if r.get(\"status\") == \"completed\"]\n",
    "output_scores = [r[\"output_score\"] for r in completed if r.get(\"output_score\") is not None]\n",
    "trajectory_scores = [r[\"trajectory_score\"] for r in completed if r.get(\"trajectory_score\") is not None]\n",
    "\n",
    "print(f\"Sessions: {len(completed)}/{len(all_session_results)} completed\")\n",
    "print(f\"Cases evaluated: {total_cases_evaluated}\")\n",
    "print(f\"CloudWatch logs sent: {total_logs_sent}\")\n",
    "\n",
    "if output_scores:\n",
    "    print(f\"Output score: avg={sum(output_scores)/len(output_scores):.2f}, min={min(output_scores):.2f}, max={max(output_scores):.2f}\")\n",
    "if trajectory_scores:\n",
    "    print(f\"Trajectory score: avg={sum(trajectory_scores)/len(trajectory_scores):.2f}, min={min(trajectory_scores):.2f}, max={max(trajectory_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세션별 결과\n",
    "\n",
    "Output 및 trajectory 점수를 보여주는 각 세션의 개별 결과입니다. \"skipped\"로 표시된 세션은 span이나 유효한 case가 없었습니다. \"error\"로 표시된 세션은 처리 중 예외가 발생했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in enumerate(all_session_results):\n",
    "    status = r.get(\"status\", \"unknown\")\n",
    "    if status == \"completed\":\n",
    "        print(f\"{i+1}. {r['session_id'][:20]}... output={r.get('output_score', 0):.2f} traj={r.get('trajectory_score') or '-'}\")\n",
    "    else:\n",
    "        print(f\"{i+1}. {r['session_id'][:20]}... {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 내보내기\n",
    "\n",
    "추가 분석이나 보고를 위해 evaluation 결과를 JSON으로 저장합니다. 내보내기에는 구성, 요약 통계 및 세션별 결과가 포함됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "export_data = {\n",
    "    \"evaluation_time\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"config\": {\n",
    "        \"source_log_group\": SOURCE_LOG_GROUP,\n",
    "        \"eval_results_log_group\": EVAL_RESULTS_LOG_GROUP,\n",
    "        \"output_evaluator\": OUTPUT_EVALUATOR_NAME,\n",
    "        \"trajectory_evaluator\": TRAJECTORY_EVALUATOR_NAME,\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"total_sessions\": len(all_session_results),\n",
    "        \"completed_sessions\": len(completed),\n",
    "        \"total_cases\": total_cases_evaluated,\n",
    "        \"avg_output_score\": sum(output_scores) / len(output_scores) if output_scores else None,\n",
    "        \"avg_trajectory_score\": sum(trajectory_scores) / len(trajectory_scores) if trajectory_scores else None,\n",
    "    },\n",
    "    \"session_results\": all_session_results,\n",
    "}\n",
    "\n",
    "with open(RESULTS_JSON_PATH, \"w\") as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"Exported to {RESULTS_JSON_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}